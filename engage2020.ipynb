{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wRgO5RfZkAP"
   },
   "source": [
    "ENGAGE 2020 - Domino and A.I.\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWvih7rrZkAU"
   },
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE8KbP7wZkAV"
   },
   "source": [
    "> Heart Disease is a major public health problem, with over 5,000,000 newly diagnosed cases in the United States every year. Melanoma is the deadliest form of skin cancer, responsible for an overwhelming majority of skin cancer deaths. In 2015, the global incidence of melanoma was estimated to be over 350,000 cases, with almost 60,000 deaths. Although the mortality is significant, when detected early, melanoma survival exceeds 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLP71xMvZkAj"
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4M26YrFZkAk"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jckLkB7BZkAy"
   },
   "source": [
    "Let's import Tensorflow now.  It is helpful after importing Tensorflow to check the version of the library installed since the API's are changing so quickly.  Several machines are now starting to use Tensorflow 2.0rc which has a very different interface from v1.* as well.  In preparation for TF 2.0, this tutorial will use the Eager Execution API, which is used by default in future version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2zeNOG0ZkA6"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okZqUwQrZkA8"
   },
   "source": [
    "### About HAM10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPCUciQaZkA-"
   },
   "source": [
    "In this exercise will will be using  HAM10000 (\"Human Against Machine with 10000 training images\") dataset which is a collection of \"dermatoscopic images from different populations, acquired and stored by different modalities.\"[Kaggle](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000) This dataset has ~10K dermatoscopic images that have been reformatted to be similar smaller like the famous Handwritten Digit or MNIST dataset (64 x 64 images centered on the subject).  This allows us to focus on setting up our environment and making sure we can get a model working on a more interesting, healthcare-related problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPtl5ciKZkBA"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riupM0POZkBB"
   },
   "source": [
    "We will load both the included meta file as well as the image data (provided conveniently as a CSV file with one row per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "2oDgrLPee3hm",
    "outputId": "9e195ce3-13ad-4ce4-a56a-8999bfe9db5a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pu58mm6FfH2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oFk745zZkBD"
   },
   "outputs": [],
   "source": [
    "# IS_ONEPANEL = True\n",
    "# NEED_DOWNLOAD = False\n",
    "# if IS_ONEPANEL:\n",
    "#     DATA_DIR='/onepanel/input/datasets/curae/skin-cancer-mnist/1'\n",
    "# else:\n",
    "#     import os\n",
    "#     import load_data\n",
    "# #     DATA_DIR = '/storage/codelab1'\n",
    "#     DATA_DIR = '/content/drive/My Drive/Skin-Cancer'\n",
    "#     if NEED_DOWNLOAD:\n",
    "#         if not os.path.exists(DATA_DIR):\n",
    "#             os.mkdir(DATA_DIR)\n",
    "#         FILELIST = ['HAM10000_metadata.csv','train.npz','val.npz']\n",
    "#         load_data.download_files(FILELIST,DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQTswzCkZkBL"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/content/drive/My Drive/Skin-Cancer'\n",
    "meta_df = pd.read_csv(os.path.join(DATA_DIR,'HAM10000_metadata.csv'))\n",
    "train_data = np.load(os.path.join(DATA_DIR,'train.npz'))\n",
    "test_data = np.load(os.path.join(DATA_DIR,'val.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_T81FI0EZkBS"
   },
   "source": [
    "### Inspect the meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAUOFlwPZkBT"
   },
   "source": [
    "#### Lesions with multiple images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mVhDmH4ZkBa"
   },
   "source": [
    "For each image we have a `lesion_id` as well as a an `image_id`.  We should note that there are in fact multiple images for the same lesion which we need to be mindful of as we construct our training and testing splits.  While this has been accounted for in the pre-made splits, it is a potential leakage between the training and validation splits if the same patients were in both populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dRyR5e2aZkBb",
    "outputId": "e9e83bc2-a82e-4840-9238-7fa1ed1416c6"
   },
   "outputs": [],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-MG8QzeZkBg"
   },
   "source": [
    "Below, we count the occurrences of a lesion_id, accumulate the lesions ids with more than 1 image and then add a flag to our original meta_df to indicate whether the lesion has more than one image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNfXw1iMZkBh"
   },
   "outputs": [],
   "source": [
    "def pretty_count_print(series):\n",
    "    display(pd.DataFrame(series.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xgg1OrhYZkBk",
    "outputId": "0ca46725-0ff2-4b11-e9b6-97155a90b126"
   },
   "outputs": [],
   "source": [
    "lesion_image_counts = pd.DataFrame( meta_df.groupby('lesion_id')['image_id'].count())\n",
    "dupe_lesion_ids = list(lesion_image_counts[lesion_image_counts['image_id'] > 1].index.values)\n",
    "meta_df['dupe'] = meta_df.apply(lambda row: row['lesion_id'] in dupe_lesion_ids, axis=1)\n",
    "pretty_count_print(meta_df['dupe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tpWkru5ZkBn"
   },
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzaUB0MoZkBo"
   },
   "source": [
    "The dataset contains 7 classes of lesions, a mixture of both benign and cancerous types.  The dataset is overweight to benign melanocytic nevi, a harmless mole.  However, there are over 2,000 examples of cancerous lesions to detect in this dataset.\n",
    "\n",
    "\n",
    "| Id |Abbr | Name |\n",
    "|---|---|---|\n",
    "| 4 | nv | Melanocytic nevi |\n",
    "| 6 | mel | Melanoma |\n",
    "| 2 | bkl | Benign keratosis-like lesion |\n",
    "| 1 | bcc | Basal cell carcinoma |\n",
    "| 0 | akiec |  Actinic keratoses |\n",
    "| 5 | vasc | Vascular lesions (angiomas, etc.) |\n",
    "| 3 | df | Dermatofibroma |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "9BzGLSLrZkBs",
    "outputId": "67c04746-f5dd-45b3-db6c-319b9aeeb880"
   },
   "outputs": [],
   "source": [
    "class_list = ['akiec','bcc','bkl','df','mel','nv','vasc']\n",
    "pretty_count_print(meta_df['dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLJgCJ4KZkBv"
   },
   "source": [
    "Let's gather examples of each class to inspect visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "KVRemMPaZkBw",
    "outputId": "1097ce57-8c8e-4c27-c559-d8519c4427f8"
   },
   "outputs": [],
   "source": [
    "example_list = {}\n",
    "class_names = ['akiec', 'df', 'bkl', 'vasc', 'nv', 'bcc', 'mel']\n",
    "for cls in range(7):\n",
    "    example_list[class_names[cls]] = np.nonzero(train_data['labels'] == cls)[0][0]\n",
    "display(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzYm6RLQZkBz",
    "outputId": "4f159073-b741-455a-fbc1-23c5a42a8e27"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,2, figsize=(20,20))\n",
    "fig.delaxes(ax[3,1]) # Delete the extra plot\n",
    "for i, (key, value) in enumerate(example_list.items()):\n",
    "    ax[i//2,i%2].title.set_text(key)\n",
    "    ax[i//2,i%2].imshow(train_data['images'][value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN36RWDwZkB2"
   },
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2pAH8NsZkB3"
   },
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUXM3B8UZkB4"
   },
   "source": [
    "A good practice is to divide your data into a training set as well as a testing set.  The neural network parameters are trained using the training dataset.  We hold out a portion of the data into a testing set where we can test the effectiveness of the model in predicting unseen data.  We have already created a 90/10 split for the test set for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQP8DwUOZkB5"
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mC7tXszZkB9"
   },
   "source": [
    "Uniquely with toy datasets like this one, there is not much in the way of preprocessing to accomplish.  We will simply scale the 8-bit color values normalizing the per-channel mean and variance.  It is common practice to only calculate these statistics only on the training population, but use the same normalization parameters for both populations during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k21ObQDMZkB_",
    "outputId": "5b4307a2-49ad-48a4-cd3e-56557ca3d83c"
   },
   "outputs": [],
   "source": [
    "channel_mean = np.mean(train_data['images'],axis=(0,1,2))\n",
    "\n",
    "channel_std = np.std(train_data['images'],axis=(0,1,2))\n",
    "\n",
    "print(\"Channel mean: %s\" % str(channel_mean))\n",
    "print(\"Channel std: %s\" % str(channel_std))\n",
    "\n",
    "train_x = (train_data['images'] - channel_mean) / channel_std\n",
    "test_x = (test_data['images'] - channel_mean) / channel_std\n",
    "print(\"Processed color range: %0.3f to %0.3f\" % (np.min(train_x),np.max(train_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlboheMjZkCC"
   },
   "source": [
    "**NOTE:** Given the imbalance of the dataset and the low resolution that we are using for demonstration purposes, we will confine ourselves to the binary classification problem of whether a lesion is a \"nevus\" or \"not a nevus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am_88IrrZkCE"
   },
   "outputs": [],
   "source": [
    "train_y = np.where(train_data['labels']==4,1,0).reshape(-1,1).astype(np.float32)\n",
    "test_y = np.where(test_data['labels']==4,1,0).reshape(-1,1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DpqgvEwZkCH",
    "outputId": "3305fb9d-2f11-4ede-da5e-db6d02cc9249"
   },
   "outputs": [],
   "source": [
    "print(\"Proportion of nevi in train: %0.2f%% vs test: %0.2f%%\" \n",
    "      % (np.mean(train_y)*100., np.mean(test_y) *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiOtaiDyZkCT"
   },
   "source": [
    "One last check before proceeding, let's check the data types and sizes of our input data are what we were expecting (9,077 and 938 training and test samples, pictures are 64x64 with 3 color channels, the labels and image data should be floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV7Wdkw-ZkCU",
    "outputId": "99036b70-55c2-4372-eb4c-4539f0a5edc8"
   },
   "outputs": [],
   "source": [
    "print(train_x.shape, train_x.dtype, train_y.shape, train_y.dtype)\n",
    "print(test_x.shape, test_x.dtype, test_y.shape, test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eumGC3JdZkCe"
   },
   "source": [
    "#### Data Loading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJwnTnStZkCf"
   },
   "source": [
    "These functions will load all of the data into the device memory in a format that the Eager Execution API works well when the data provided is via `tf.data.Dataset` which helps create an interable collection of tensors.\n",
    "\n",
    "These dataloading functions will automatically keep track of shuffling and returning batches of data in the appropriate size.  We can also add an augmentation step to our training dataset loading pipeline by adding a `map` with an augmentation function.\n",
    "\n",
    "For augmentation, we only apply random flips (both horizontal and vertical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAqky0j7ZkCn"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wpxpRW9ZkCq"
   },
   "outputs": [],
   "source": [
    "def augment(images, labels):\n",
    "    images = tf.image.random_flip_left_right(images)\n",
    "    images = tf.image.random_flip_up_down(images)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wg2juPuAZkCw"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_dataset = train_dataset.map(augment).shuffle(2000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYm1PdNuZkC0"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sB4c7WISZkC3"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRCgaIJpZkC4"
   },
   "source": [
    "Now we will construct the model using the modular Keras API, which will become the preferred method for building models in Tensorflow 2.0.  Specifically we will create a new model class that subclasses `tf.keras.Model` to incorporate our model design.  This helps with organizing the state of our graph as well as helps make models easier to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IL5cStLjZkC6"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.activations import sigmoid, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHJ_hAPtZkDA"
   },
   "source": [
    "### Design Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-2D9NPtZkDB"
   },
   "source": [
    "There are a number of parameters required when building the model.  Below, we collect the parameters required in the model in the `model_design_params` nested dictionary.  The structure of the dictionary largely follows the design of the VGG-like binary classifier CNN model that we will use to detect whether a lesion is a nevus or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2ojK-dSZkDB"
   },
   "outputs": [],
   "source": [
    "model_design_params = {\n",
    "    # Parameters for blocks\n",
    "    'blocks' : {\n",
    "         # Parameters for the 2D convolutional layers\n",
    "         'conv2d': {\n",
    "             'kernel_size': 3,\n",
    "             'strides': 1,\n",
    "             'padding':'same',\n",
    "             'activation':'relu',\n",
    "          },\n",
    "         # Parameters for the 2D max pooling layers\n",
    "         'pooling': {\n",
    "             'pool_size': 2,\n",
    "             'strides': 2,\n",
    "          },\n",
    "         # Dropout hyperparameter for the blocks\n",
    "         'dropout': 0.25,\n",
    "     },\n",
    "     # Parameters for the final head\n",
    "     'head' : {\n",
    "         # Use one final affine layer before classifying\n",
    "         'affine' : {\n",
    "             'units': 512,\n",
    "             'activation': 'relu',\n",
    "         },\n",
    "         # With dropout\n",
    "         'dropout': 0.3,\n",
    "         # Final output settings, note we are not using an activation function, it means we are returning logits\n",
    "         'logits' : {\n",
    "             'units': 1,\n",
    "             'activation': None,\n",
    "         },   \n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUIT7IcAZkDZ"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0DmGhNnZkDa"
   },
   "source": [
    "Our model is going to be of the style of a sequential VGG  model.  These models are composed a t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXflSdTkZkDa"
   },
   "outputs": [],
   "source": [
    "class VGGStyleBlock(Model):\n",
    "    def __init__(self, filters, params):\n",
    "        '''\n",
    "        :param filters: number of channels in the 2D convolutional layers\n",
    "        :param params: dictionary of block settings described above\n",
    "        '''\n",
    "        super(VGGStyleBlock, self).__init__()\n",
    "        self.conv1 =  Conv2D(filters, **params['conv2d']) # +2 RF\n",
    "        self.conv2 =  Conv2D(filters, **params['conv2d']) # +2 RF\n",
    "        self.maxpool = MaxPooling2D(**params['pooling']) # 2x+1 RF\n",
    "        self.dropout = Dropout(params['dropout'])\n",
    "        self.sequential =   Sequential([self.conv1,\n",
    "                                        self.conv2,\n",
    "                                        self.maxpool,\n",
    "                                        self.dropout]) # put it all together\n",
    "    def call(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFxpKJcuZkEC"
   },
   "outputs": [],
   "source": [
    "class NevusCNNClassifier(Model):\n",
    "  def __init__(self,params):\n",
    "    super(NevusCNNClassifier, self).__init__()\n",
    "    \n",
    "    # Block 1: 64x64x3 -> 32x32x32\n",
    "    self.block1 = VGGStyleBlock(filters=32,params=model_design_params['blocks'])\n",
    "    \n",
    "    # Block 2: 32x32x32 -> 16x16x64\n",
    "    self.block2 = VGGStyleBlock(filters=64,params=model_design_params['blocks'])\n",
    "    \n",
    "    # Block 3: 16x16x64 -> 8x8x128\n",
    "    self.block3 = VGGStyleBlock(filters=128,params=model_design_params['blocks'])\n",
    "\n",
    "    # Features 8x8x128 -> 128 vector representations\n",
    "    self.gap =      GlobalAveragePooling2D()\n",
    "    self.features = Sequential([self.block1,\n",
    "                                self.block2,\n",
    "                                self.block3,\n",
    "                                self.gap])\n",
    "    \n",
    "    # Head\n",
    "    self.affine =   Dense(**params['head']['affine'])\n",
    "    self.head_dropout = Dropout(params['head']['dropout'])\n",
    "    self.logits =   Dense(**params['head']['logits'])\n",
    "    self.head =     Sequential([self.affine,\n",
    "                                self.head_dropout, \n",
    "                                self.logits])\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.features(x)\n",
    "    logits = self.head(x)\n",
    "    probability  = sigmoid(logits)\n",
    "    return logits, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBL5d1rSZkEE"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = NevusCNNClassifier(model_design_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0se8UvROZkEH"
   },
   "source": [
    "Now that we've constructed the models.  Let's inspect the models now.  Keras comes with two handy built-in features for inspecting models.  Specfically, `model.summary()` and `keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNhqoeq7ZkEH",
    "outputId": "3d813dd0-75d5-4a92-be4b-cffaf8f719d7"
   },
   "outputs": [],
   "source": [
    "tmp = NevusCNNClassifier(model_design_params)\n",
    "tmp.build((1,64,64,3))\n",
    "tmp.features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_k8WGc-ZkEM",
    "outputId": "e28df12a-3d4a-41e6-a39f-27466ae50230"
   },
   "outputs": [],
   "source": [
    "tmp.head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1sqYNdPZkEO"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXwrS3q7ZkES"
   },
   "source": [
    "No we can finally start to train our model after some final bookkeeping items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gfc7chvgZkEV"
   },
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzgYBZ_uZkEW"
   },
   "source": [
    "Now we need to set some key training hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIlk-BaiZkEX"
   },
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryBOo8vfZkEX"
   },
   "source": [
    "We train our model by tuning the parameters of the network until the prediction error or loss converges to a stable value.  Since we are training a binary classifier we will use Sigmoid Cross Entropy or Binary Cross Entropy.  This is closely related to logistic regression.\n",
    "\n",
    "We will use the Adam optimizer to perform a variation on stochastic gradient descent.  The primary improvement of Adam over normal SGD is that Adam uses adaptive learning rates for its weight updates that depend on the size of the gradients as well as the size of the weights being updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWlBWK56ZkEY",
    "outputId": "3c211982-5da6-4b58-e615-84aa95d0112c"
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "global_step.assign(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gaQUiX8HZkEa"
   },
   "outputs": [],
   "source": [
    "INITIAL_LEARNING_RATE = 1.e-4\n",
    "FIRST_DECAY_STEPS = 1000\n",
    "CLIPVALUE = 1.\n",
    "EPOCHS = 20\n",
    "\n",
    "lr_scheduler = tf.train.cosine_decay_restarts(\n",
    "    INITIAL_LEARNING_RATE,\n",
    "    global_step,\n",
    "    FIRST_DECAY_STEPS,\n",
    "    t_mul=2.0,\n",
    "    m_mul=1.0,\n",
    "    alpha=0.0,\n",
    ")\n",
    "\n",
    "# optimizer = tf.train.MomentumOptimizerOptimizer(learning_rate=LEARNING_RATE,momentum=0.9)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_scheduler)\n",
    "loss_function = tf.losses.sigmoid_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ffYyhpQZkEc"
   },
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V5X7yUEZkEd"
   },
   "source": [
    "We will track the performance of our model during training using three metrics in addition to tracking our prediction error (loss).  Specifically, we will use 1) binary accuracy, 2) precision and 3) recall.  We will track these metrics in a stateful dictionary for both our training sets and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI7cpAqLZkEd"
   },
   "outputs": [],
   "source": [
    "train_metrics = {} \n",
    "train_metrics['loss'] = tf.keras.metrics.Mean(name='train/loss')\n",
    "train_metrics['accuracy'] = tf.keras.metrics.BinaryAccuracy(name='train/accuracy')\n",
    "train_metrics['precision'] = tf.keras.metrics.Precision(name='train/precision')\n",
    "train_metrics['recall'] = tf.keras.metrics.Recall(name='train/recall')\n",
    "\n",
    "test_metrics = {} \n",
    "test_metrics['loss'] = tf.keras.metrics.Mean(name='test/loss')\n",
    "test_metrics['accuracy'] = tf.keras.metrics.BinaryAccuracy(name='test/accuracy')\n",
    "test_metrics['precision'] = tf.keras.metrics.Precision(name='test/precision')\n",
    "test_metrics['recall'] = tf.keras.metrics.Recall(name='test/recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dhwbww37ZkEk"
   },
   "outputs": [],
   "source": [
    "# Helper for printing the metrics\n",
    "def print_metrics(metric_dict, mode='Train'):\n",
    "    print(mode,end=' ')\n",
    "    for key, value in metric_dict.items():\n",
    "        print(\" %s: %0.3f\" % (key, value.result()),end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOvg_xFDZkEp"
   },
   "outputs": [],
   "source": [
    "# Helper for updating the metrics\n",
    "def update_metrics(metric_dict,loss, labels, preds):\n",
    "    metric_dict['loss'].update_state(loss)\n",
    "    metric_dict['accuracy'].update_state(labels, preds)\n",
    "    metric_dict['precision'].update_state(labels, preds)\n",
    "    metric_dict['recall'].update_state(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PMlL2yXZkEs"
   },
   "outputs": [],
   "source": [
    "# Helper for reseting the metrics after each epoch\n",
    "def reset_metrics(metric_dict):\n",
    "    for metric_object in metric_dict.values():\n",
    "        metric_object.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XR0ceegZkEu"
   },
   "source": [
    "#### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SUQf3ImZkEv"
   },
   "source": [
    "During a training cycle for a single batch of images and labels, we will:\n",
    "1. Generate a forward pass classification for the training images based on the most current model parameters.\n",
    "2. During this forward pass, we track which variables in the graph are changing and calculate our error while in the GradientTape context.\n",
    "3. We then calculate the gradients of the loss with respect to the model's trainable variables.\n",
    "4. We then use our Adam optimizer to perform a weight update step, hopefully updating a model reduce its training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_n-fHB4ZkEw"
   },
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits, probability = model(images)\n",
    "    loss = loss_function(labels, logits)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  update_metrics(train_metrics,loss,labels,probability)\n",
    "  return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0goRyJ8WZkE6"
   },
   "source": [
    "Since we are using eager execution, it is easy for us to treat our training cycle like any python function.  Let's run a few steps to make sure everything is working so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMqgpOLlZkE7",
    "outputId": "6be8e842-23a5-4fa6-d9b9-32568561c1e7"
   },
   "outputs": [],
   "source": [
    "for image, label in train_dataset.take(10):\n",
    "    train_step(image,label)\n",
    "    print_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAYR5JI1ZkE_"
   },
   "source": [
    "#### Evaluation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-4ED7AXZkFA"
   },
   "source": [
    "To make sure our model training is keeping on track and can still generalize to unseen data, it is useful to calculate the error and performance metrics on a hold out set in an alternating fashion with our training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVR9gmoRZkFD"
   },
   "outputs": [],
   "source": [
    "def test_step(images, labels):\n",
    "    t_logits, t_probability = model(images)\n",
    "    t_loss = loss_function(labels, t_logits)\n",
    "    \n",
    "    update_metrics(test_metrics,t_loss,labels, t_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgYXWSWgZkFH"
   },
   "source": [
    "We can take a single evaluation step now to make sure that this function and data set are wired correctly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7-xI1uVZkFH",
    "outputId": "fe9ce9ff-3409-4d1c-c1c7-71b19ba2ca9b"
   },
   "outputs": [],
   "source": [
    "for image, label in test_dataset.take(1):\n",
    "    test_step(image,label)\n",
    "    print_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iljTg0wuZkFK"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u5bsLXmZkFg"
   },
   "source": [
    "Putting this all together, let's run our training step for several cycles until our model's error converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEfD36e2ZkFh"
   },
   "source": [
    "Another helpful tool when training a Tensorflow model is to use Tensorboard to a visualize training progress.  Let's create a directory for our Tensorboard outputs and create a writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGuR1QVPZkFh"
   },
   "outputs": [],
   "source": [
    "# Update logdir to be a different location to keep track of old runs\n",
    "logdir = \"/onepanel/output/model2\"\n",
    "writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImNyPt39ZkFm"
   },
   "source": [
    "Another helpful practice is to persist checkpoints of our model every so often.  Tensorflow allows us to save the current model weights, optimizer values as well as the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TNTvyvlZkFm",
    "outputId": "7246816e-945c-4402-8f8d-badf7a8c38f0"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = logdir\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                           model=model,\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "root.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPj54jPkZkFo"
   },
   "source": [
    "Now for the fun part, we push 'RUN' and wait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnS1Q7EtZkFp",
    "outputId": "d3c174f8-7dea-488d-e76b-3ce39ad99b5f"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_dataset:\n",
    "        global_step.assign_add(1)\n",
    "\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
    "            preds = train_step(images, labels)\n",
    "            for metric, val in train_metrics.items():\n",
    "                tf.contrib.summary.scalar('train/'+ metric, val.result())\n",
    "                tf.contrib.summary.histogram('train/predictions',preds)\n",
    "            tf.contrib.summary.scalar('learning_rate', lr_scheduler())\n",
    "\n",
    "    \n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "    with tf.contrib.summary.always_record_summaries():\n",
    "        for metric, val in test_metrics.items():\n",
    "            tf.contrib.summary.scalar('test/'+ metric, val.result())\n",
    "\n",
    "    print(\"Epoch {} ==================\".format(epoch))\n",
    "    print_metrics(train_metrics)\n",
    "    print_metrics(test_metrics, \"Test\")\n",
    "    # Reset the metrics for the next epoch\n",
    "    reset_metrics(train_metrics)\n",
    "    reset_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QFR6W-gZkF8"
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfiLWECUZkF8"
   },
   "source": [
    "Since we have included calls to our `tf.summary.Writer` that we set up for `model1` log directory, we can watch our model training with finer grained statistics in TensorBoard.  We will now demo getting into the Tensorboard interface now.\n",
    "\n",
    "Hopefully, we see our losses for both our training set and our held-out test set declining to convergence.  Also, we should see our accuracy, precision and recall climb to a stable number during training.  However, in using a learning rate rescheduler with restarts, the metrics can get a little bumpy around the times when the learning rate jumpbs back up.\n",
    "\n",
    "If we start to see our training performance parameters greatly outpacing our training performance, then we may be overfitting to the training data, and our model may not generalize well to unseen skin lesions in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9dIIQQcZkF9"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYWsU1RjZkF-"
   },
   "source": [
    "### Score the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE-DvitoZkF_"
   },
   "outputs": [],
   "source": [
    "_ , test_preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RY8NdDGuZkGB"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.4\n",
    "test_class = np.where(test_preds.numpy() > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-XnIhAaZkGF",
    "outputId": "e473477b-0563-4c12-c3fa-945808171a35"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "display(sklearn.metrics.classification_report(test_y,test_class, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CVlhlMOZkGM"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function \n",
    "    s and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    FROM: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[sklearn.utils.multiclass.unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwcYZiM8ZkGO",
    "outputId": "3756722e-ee37-475a-d645-fd39bf6989c4"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_y,test_class,['No Nevus','Nevus'],title='Confusion Matrix for Nevus Binary Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ntQOeIJZkGR"
   },
   "source": [
    "### Inspect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58D64melZkGR"
   },
   "outputs": [],
   "source": [
    "wrong_mask = (test_y != test_class).reshape(-1)\n",
    "wrong_labels = test_data['labels'][wrong_mask]\n",
    "wrong_preds = test_class[wrong_mask]\n",
    "wrong_images = test_x[wrong_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M58HC7TYZkGg",
    "outputId": "c498eaf0-4727-4e89-d683-b8aaa0e86906"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,2, figsize=(20,20))\n",
    "for i, (label, pred, image_std) in enumerate(zip(wrong_labels[:10],wrong_preds[:10],wrong_images[:10])):\n",
    "    label_txt = class_names[label]\n",
    "    pred_txt = 'No Nevus' if pred == 0 else 'Nevus'\n",
    "    original_image = (image_std * channel_std +  channel_mean).astype(np.uint8)\n",
    "    ax[i//2,i%2].title.set_text('Predicted: %s, True: %s' % (pred_txt, label_txt))\n",
    "    ax[i//2,i%2].imshow(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNBc8N3AZkGj"
   },
   "source": [
    "### Inspect feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n68yxWNSZkGk"
   },
   "outputs": [],
   "source": [
    "test_features = model.features(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVKWJEADZkGm"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "test_features_2d = pca.fit(test_features).transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaI3G4_HZkGq"
   },
   "outputs": [],
   "source": [
    "nevus_mask, no_nevus_mask = (test_y == 1).reshape(-1), (test_y == 0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4E3xL-I6ZkG-",
    "outputId": "a53ee6f5-9cb3-4005-89e9-bd477d026cf2"
   },
   "outputs": [],
   "source": [
    "plt.scatter(test_features_2d[nevus_mask,0],test_features_2d[nevus_mask,1],color='r')\n",
    "plt.scatter(test_features_2d[no_nevus_mask,0],test_features_2d[no_nevus_mask,1], color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBhlAQM5ZkHB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Skin-cancer-mnist_tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
